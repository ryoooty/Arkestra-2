junior:
  provider: "llama-cpp"
  model_path: ".\\models\\gemma3n-e4b\\google_gemma-3n-E4B-it-Q4_K_M.gguf"
  temperature: 0.2
  max_new_tokens: 64
  stop: ["\n\n", "```"]
senior:
  model_id: "mistral-7b"
  temperature: 0.7
  max_new_tokens: 512
  endpoint: "https://api.your-llm-provider.example/v1/chat/completions"
