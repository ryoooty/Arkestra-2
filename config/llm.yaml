junior:
  provider: transformers
  model_id: meta-llama/Llama-3.2-1B-Instruct
  load_in_4bit: true
  torch_dtype: float16
  device_map: auto
  max_new_tokens: 160
  temperature: 0.2
  stop: ["</json>"]
  use_chat_template: true
senior:
  provider: "transformers"
  model_path: "C:\\Users\\kosha\\models\\mistral-7b-instruct-v0_3"
  temperature: 0.7
  max_new_tokens: 512
  stop: ["\n\n"]
